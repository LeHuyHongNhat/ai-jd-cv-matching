# Load Testing cho CV-JD Matching API

H∆∞·ªõng d·∫´n chi ti·∫øt c√°ch th·ª±c hi·ªán Load Testing v√† Stress Testing cho h·ªá th·ªëng CV-JD Matching.

## M·ª•c l·ª•c

- [T·ªïng quan](#t·ªïng-quan)
- [C√°c ph∆∞∆°ng ph√°p Load Testing](#c√°c-ph∆∞∆°ng-ph√°p-load-testing)
- [Chu·∫©n b·ªã](#chu·∫©n-b·ªã)
- [Ph∆∞∆°ng ph√°p 1: Locust](#ph∆∞∆°ng-ph√°p-1-locust-khuy√™n-d√πng)
- [Ph∆∞∆°ng ph√°p 2: Simple Load Test](#ph∆∞∆°ng-ph√°p-2-simple-load-test)
- [Ph∆∞∆°ng ph√°p 3: Apache Bench](#ph∆∞∆°ng-ph√°p-3-apache-bench)
- [Ph∆∞∆°ng ph√°p 4: k6](#ph∆∞∆°ng-ph√°p-4-k6)
- [ƒê·ªçc v√† ph√¢n t√≠ch k·∫øt qu·∫£](#ƒë·ªçc-v√†-ph√¢n-t√≠ch-k·∫øt-qu·∫£)
- [Best Practices](#best-practices)

---

## T·ªïng quan

Load Testing gi√∫p b·∫°n:

- **X√°c ƒë·ªãnh kh·∫£ nƒÉng ch·ªãu t·∫£i**: H·ªá th·ªëng c√≥ th·ªÉ x·ª≠ l√Ω bao nhi√™u request ƒë·ªìng th·ªùi?
- **T√¨m bottleneck**: Ph·∫ßn n√†o c·ªßa h·ªá th·ªëng b·ªã ngh·∫Ωn tr∆∞·ªõc?
- **ƒêo performance**: Response time, throughput, error rate
- **Ki·ªÉm tra stability**: H·ªá th·ªëng c√≥ ·ªïn ƒë·ªãnh khi ch·ªãu t·∫£i cao?

## C√°c ph∆∞∆°ng ph√°p Load Testing

| C√¥ng c·ª•           | ƒê·ªô kh√≥     | Giao di·ªán      | T√≠nh nƒÉng                     | Khuy√™n d√πng |
| ----------------- | ---------- | -------------- | ----------------------------- | ----------- |
| **Locust**        | D·ªÖ         | ‚úÖ Web UI      | Ph√¢n t√°n, real-time, flexible | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê  |
| **Simple Script** | R·∫•t d·ªÖ     | ‚ùå Console     | ƒê∆°n gi·∫£n, nhanh               | ‚≠ê‚≠ê‚≠ê‚≠ê    |
| **Apache Bench**  | D·ªÖ         | ‚ùå Console     | Nhanh, lightweight            | ‚≠ê‚≠ê‚≠ê      |
| **k6**            | Trung b√¨nh | ‚ùå Console     | Modern, nhi·ªÅu t√≠nh nƒÉng       | ‚≠ê‚≠ê‚≠ê‚≠ê    |
| **JMeter**        | Kh√≥        | ‚úÖ Desktop GUI | ƒê·∫ßy ƒë·ªß, enterprise            | ‚≠ê‚≠ê‚≠ê      |

---

## Chu·∫©n b·ªã

### 1. Kh·ªüi ƒë·ªông API Server

Tr∆∞·ªõc khi test, ƒë·∫£m b·∫£o server ƒëang ch·∫°y:

```bash
# Terminal 1: Kh·ªüi ƒë·ªông server
uvicorn app.api.main:app --reload --host 0.0.0.0 --port 8000
```

Ki·ªÉm tra server:

```bash
curl http://localhost:8000/
```

### 2. C√†i ƒë·∫∑t Dependencies

```bash
# Cho Locust
pip install locust

# Cho Simple Load Test
pip install requests

# Cho Apache Bench (Linux)
sudo apt-get install apache2-utils

# Cho Apache Bench (Mac)
brew install httpd

# Cho k6 (Linux/Mac)
# Xem h∆∞·ªõng d·∫´n t·∫°i: https://k6.io/docs/getting-started/installation/
```

---

## Ph∆∞∆°ng ph√°p 1: Locust (Khuy√™n d√πng)

### ∆Øu ƒëi·ªÉm

‚úÖ Giao di·ªán web ƒë·∫πp, tr·ª±c quan  
‚úÖ Real-time charts v√† statistics  
‚úÖ D·ªÖ vi·∫øt test scenarios b·∫±ng Python  
‚úÖ H·ªó tr·ª£ distributed testing  
‚úÖ Export reports (HTML, CSV)

### C√†i ƒë·∫∑t

```bash
pip install locust
```

### Ch·∫°y Test

#### C√°ch 1: Web Interface (Recommended)

```bash
# Ch·∫°y Locust v·ªõi Web UI
locust -f load_test_locust.py --host=http://localhost:8000

# M·ªü tr√¨nh duy·ªát: http://localhost:8089
```

Trong Web UI:

1. **Number of users**: S·ªë l∆∞·ª£ng users ƒë·ªìng th·ªùi (vd: 100)
2. **Spawn rate**: T·ªëc ƒë·ªô tƒÉng users/gi√¢y (vd: 10 users/s)
3. Click **Start swarming**

#### C√°ch 2: Headless (No UI)

```bash
# Test v·ªõi 100 users, tƒÉng 10 users/s, ch·∫°y trong 60s
locust -f load_test_locust.py --host=http://localhost:8000 \
    --users 100 --spawn-rate 10 --run-time 60s --headless

# Export HTML report
locust -f load_test_locust.py --host=http://localhost:8000 \
    --users 100 --spawn-rate 10 --run-time 60s --headless \
    --html=load_test_report.html
```

### Test Scenarios trong Locust

File `load_test_locust.py` bao g·ªìm:

- ‚úÖ **test_root_endpoint**: Test GET / (10% traffic)
- ‚úÖ **test_process_jd**: Test POST /process/jd (30% traffic)
- ‚úÖ **test_process_cv_simulation**: Test POST /process/cv (30% traffic)
- ‚úÖ **test_match_cv_jd**: Test GET /match/{cv_id}/{jd_id} (30% traffic)

### V√≠ d·ª• Test Progressively

```bash
# B∆∞·ªõc 1: Warmup - 10 users
locust -f load_test_locust.py --host=http://localhost:8000 \
    --users 10 --spawn-rate 5 --run-time 30s --headless

# B∆∞·ªõc 2: Light Load - 50 users
locust -f load_test_locust.py --host=http://localhost:8000 \
    --users 50 --spawn-rate 10 --run-time 60s --headless

# B∆∞·ªõc 3: Medium Load - 100 users
locust -f load_test_locust.py --host=http://localhost:8000 \
    --users 100 --spawn-rate 20 --run-time 120s --headless

# B∆∞·ªõc 4: Heavy Load - 500 users
locust -f load_test_locust.py --host=http://localhost:8000 \
    --users 500 --spawn-rate 50 --run-time 300s --headless

# B∆∞·ªõc 5: Stress Test - 1000 users
locust -f load_test_locust.py --host=http://localhost:8000 \
    --users 1000 --spawn-rate 100 --run-time 600s --headless
```

---

## Ph∆∞∆°ng ph√°p 2: Simple Load Test

### ∆Øu ƒëi·ªÉm

‚úÖ Kh√¥ng c·∫ßn c√†i th√™m th∆∞ vi·ªán ƒë·∫∑c bi·ªát  
‚úÖ Code ƒë∆°n gi·∫£n, d·ªÖ customize  
‚úÖ Statistics chi ti·∫øt (percentiles, std dev)  
‚úÖ Interactive menu

### Ch·∫°y Test

```bash
python load_test_simple.py
```

Ch·ªçn scenario t·ª´ menu:

```
1. Warmup - Root Endpoint (50 requests, 5 workers)
2. Light Load - Root Endpoint (100 requests, 10 workers)
3. Medium Load - Root Endpoint (500 requests, 25 workers)
4. Heavy Load - Root Endpoint (1000 requests, 50 workers)
5. Process JD - Light Load (50 requests, 5 workers)
6. Process JD - Medium Load (100 requests, 10 workers)
7. Run ALL scenarios
```

### Customize Test

M·ªü file `load_test_simple.py` v√† th√™m scenario m·ªõi:

```python
scenarios.append({
    "name": "Custom Test",
    "function": tester.test_root_endpoint,
    "requests": 2000,  # T·ªïng s·ªë requests
    "workers": 100     # Concurrent workers
})
```

---

## Ph∆∞∆°ng ph√°p 3: Apache Bench

### ∆Øu ƒëi·ªÉm

‚úÖ R·∫•t nhanh v√† nh·∫π  
‚úÖ C√≥ s·∫µn tr√™n nhi·ªÅu h·ªá ƒëi·ªÅu h√†nh  
‚úÖ D·ªÖ s·ª≠ d·ª•ng

### Nh∆∞·ª£c ƒëi·ªÉm

‚ùå Ch·ªâ test ƒë∆∞·ª£c 1 endpoint m·ªói l·∫ßn  
‚ùå Kh√¥ng c√≥ giao di·ªán ƒë·∫πp  
‚ùå √çt t√≠nh nƒÉng n√¢ng cao

### Ch·∫°y Test

#### Linux/Mac:

```bash
chmod +x load_test_ab.sh
./load_test_ab.sh
```

#### Manual Commands:

```bash
# Test Root Endpoint - 1000 requests, 50 concurrent
ab -n 1000 -c 50 http://localhost:8000/

# Test v·ªõi POST request
# T·∫°o file data
echo '{"text":"Job Description..."}' > jd_data.json

# Ch·∫°y test
ab -n 100 -c 10 -p jd_data.json -T application/json \
   http://localhost:8000/process/jd
```

---

## Ph∆∞∆°ng ph√°p 4: k6

### ∆Øu ƒëi·ªÉm

‚úÖ Modern, performance cao  
‚úÖ Script b·∫±ng JavaScript  
‚úÖ Nhi·ªÅu t√≠nh nƒÉng n√¢ng cao (thresholds, checks)  
‚úÖ Cloud integration

### C√†i ƒë·∫∑t

```bash
# Mac
brew install k6

# Linux
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
echo "deb https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
sudo apt-get update
sudo apt-get install k6
```

### Script v√≠ d·ª•

T·∫°o file `load_test_k6.js`:

```javascript
import http from "k6/http";
import { check, sleep } from "k6";

export let options = {
  stages: [
    { duration: "30s", target: 20 }, // Ramp up to 20 users
    { duration: "1m", target: 50 }, // Stay at 50 users
    { duration: "30s", target: 0 }, // Ramp down to 0 users
  ],
  thresholds: {
    http_req_duration: ["p(95)<500"], // 95% requests must complete below 500ms
    http_req_failed: ["rate<0.01"], // Error rate must be below 1%
  },
};

export default function () {
  // Test root endpoint
  let res = http.get("http://localhost:8000/");
  check(res, {
    "status is 200": (r) => r.status === 200,
    "response time < 500ms": (r) => r.timings.duration < 500,
  });

  sleep(1);

  // Test process JD
  let payload = JSON.stringify({
    text: "Job Title: Python Developer\nSkills: Python, FastAPI",
  });

  let params = {
    headers: { "Content-Type": "application/json" },
  };

  res = http.post("http://localhost:8000/process/jd", payload, params);
  check(res, {
    "JD processed successfully": (r) => r.status === 200,
  });

  sleep(1);
}
```

### Ch·∫°y k6 Test

```bash
# Ch·∫°y test
k6 run load_test_k6.js

# Ch·∫°y v·ªõi nhi·ªÅu users h∆°n
k6 run --vus 100 --duration 30s load_test_k6.js

# Export results
k6 run --out json=test_results.json load_test_k6.js
```

---

## ƒê·ªçc v√† ph√¢n t√≠ch k·∫øt qu·∫£

### C√°c ch·ªâ s·ªë quan tr·ªçng

#### 1. **Request per Second (RPS / Throughput)**

- S·ªë l∆∞·ª£ng requests h·ªá th·ªëng x·ª≠ l√Ω ƒë∆∞·ª£c m·ªói gi√¢y
- **C√†ng cao c√†ng t·ªët**
- V√≠ d·ª•: 1000 RPS = h·ªá th·ªëng x·ª≠ l√Ω 1000 requests/gi√¢y

#### 2. **Response Time (Latency)**

- Th·ªùi gian t·ª´ khi g·ª≠i request ƒë·∫øn khi nh·∫≠n response
- **C√†ng th·∫•p c√†ng t·ªët**
- Quan t√¢m ƒë·∫øn:
  - **Mean**: Trung b√¨nh
  - **Median (p50)**: 50% requests nhanh h∆°n gi√° tr·ªã n√†y
  - **p95**: 95% requests nhanh h∆°n gi√° tr·ªã n√†y
  - **p99**: 99% requests nhanh h∆°n gi√° tr·ªã n√†y

#### 3. **Error Rate**

- T·ª∑ l·ªá requests b·ªã l·ªói
- **N√™n < 1%**
- Ph√¢n lo·∫°i l·ªói:
  - 4xx: Client errors
  - 5xx: Server errors
  - Timeout

#### 4. **Concurrent Users**

- S·ªë l∆∞·ª£ng users ƒë·ªìng th·ªùi h·ªá th·ªëng c√≥ th·ªÉ handle
- T√¨m "breaking point" - ƒëi·ªÉm m√† error rate tƒÉng ƒë·ªôt ng·ªôt

### V√≠ d·ª• ƒë·ªçc k·∫øt qu·∫£ Locust

```
Type     Name                    # reqs    # fails   Avg      Min      Max    Median   req/s
------------------------------------------------------------------------
GET      /                       10000     0(0.00%)  45       12       234    43       166.67
POST     /process/jd             5000      10(0.20%) 1250     500      3000   1200     83.33
GET      /match/{cv}/{jd}        3000      5(0.17%)  850      200      2500   800      50.00
------------------------------------------------------------------------
Total                            18000     15(0.08%) 715      12       3000   600      300.00

95%ile   99%ile
--------
120      180
2100     2800
1800     2300
--------
1500     2500
```

**Ph√¢n t√≠ch:**

- ‚úÖ **RPS**: 300 requests/s - Kh√° t·ªët
- ‚úÖ **Error Rate**: 0.08% - R·∫•t th·∫•p, t·ªët
- ‚ö†Ô∏è **Response Time**:
  - Root endpoint: T·ªët (45ms trung b√¨nh)
  - Process JD: Ch·∫≠m (1250ms) - C·∫ßn optimize
  - Match: Ch·∫•p nh·∫≠n ƒë∆∞·ª£c (850ms)
- ‚ö†Ô∏è **p99**: 2500ms - C√≥ m·ªôt s·ªë requests r·∫•t ch·∫≠m

### Khi n√†o c·∫ßn lo ng·∫°i?

üö® **Warning Signs:**

- Error rate > 1%
- p95 response time > 3 seconds
- Response time tƒÉng ƒë·ªôt ng·ªôt khi tƒÉng load
- Memory leak (RAM tƒÉng li√™n t·ª•c)
- CPU usage = 100% li√™n t·ª•c

---

## Best Practices

### 1. Test Progressively (TƒÉng d·∫ßn)

```
Warmup (10 users) ‚Üí Light (50 users) ‚Üí Medium (100 users)
‚Üí Heavy (500 users) ‚Üí Stress (1000+ users)
```

**M·ª•c ƒë√≠ch**: T√¨m breaking point m√† kh√¥ng l√†m crash server ngay

### 2. Test c√°c Scenarios kh√°c nhau

- **Smoke Test**: 1-2 users, ki·ªÉm tra API ho·∫°t ƒë·ªông
- **Load Test**: Users b√¨nh th∆∞·ªùng, ki·ªÉm tra performance
- **Stress Test**: V∆∞·ª£t qu√° capacity, t√¨m breaking point
- **Spike Test**: TƒÉng ƒë·ªôt ng·ªôt users, ki·ªÉm tra elasticity
- **Soak Test**: Ch·∫°y l√¢u (v√†i gi·ªù), t√¨m memory leaks

### 3. Monitor h·ªá th·ªëng trong khi test

```bash
# CPU & Memory
htop

# API Server logs
tail -f logs/api.log

# Docker stats (n·∫øu d√πng Docker)
docker stats
```

### 4. Test tr√™n m√¥i tr∆∞·ªùng gi·ªëng Production

- C√πng hardware specs
- C√πng network latency
- C√πng database size
- C√πng c·∫•u h√¨nh

### 5. L·∫∑p l·∫°i tests

- Test nhi·ªÅu l·∫ßn ƒë·ªÉ c√≥ k·∫øt qu·∫£ ch√≠nh x√°c
- Test v√†o c√°c th·ªùi ƒëi·ªÉm kh√°c nhau trong ng√†y
- So s√°nh k·∫øt qu·∫£ sau m·ªói l·∫ßn optimize

---

## T·ªëi ∆∞u h√≥a d·ª±a tr√™n k·∫øt qu·∫£

### N·∫øu Response Time cao

1. **Profile code**: T√¨m ph·∫ßn code ch·∫≠m

   ```python
   import cProfile
   cProfile.run('your_function()')
   ```

2. **Optimize database queries**

   - Add indexes
   - Use caching (Redis)
   - Connection pooling

3. **Use async/await** (FastAPI ƒë√£ support)

4. **Cache OpenAI responses** (n·∫øu c√≥ requests gi·ªëng nhau)

### N·∫øu RPS th·∫•p

1. **Increase workers**

   ```bash
   uvicorn app.api.main:app --workers 4
   ```

2. **Use Gunicorn**

   ```bash
   pip install gunicorn
   gunicorn -w 4 -k uvicorn.workers.UvicornWorker app.api.main:app
   ```

3. **Horizontal scaling**: Deploy nhi·ªÅu instances + Load Balancer

### N·∫øu Error Rate cao

1. **Check logs**: Xem l·ªói g√¨ x·∫£y ra
2. **Add error handling**: Try-except, timeouts
3. **Add rate limiting**: Protect API kh·ªèi abuse
4. **Scale infrastructure**: TƒÉng resources

---

## V√≠ d·ª• Full Test Flow

```bash
# B∆∞·ªõc 1: Kh·ªüi ƒë·ªông server
uvicorn app.api.main:app --reload --host 0.0.0.0 --port 8000

# B∆∞·ªõc 2: Smoke test (ki·ªÉm tra c∆° b·∫£n)
curl http://localhost:8000/

# B∆∞·ªõc 3: Light load test v·ªõi Locust
locust -f load_test_locust.py --host=http://localhost:8000 \
    --users 50 --spawn-rate 10 --run-time 60s --headless \
    --html=report_light.html

# B∆∞·ªõc 4: Analyze k·∫øt qu·∫£, optimize n·∫øu c·∫ßn

# B∆∞·ªõc 5: Medium load test
locust -f load_test_locust.py --host=http://localhost:8000 \
    --users 100 --spawn-rate 20 --run-time 120s --headless \
    --html=report_medium.html

# B∆∞·ªõc 6: Heavy load test
locust -f load_test_locust.py --host=http://localhost:8000 \
    --users 500 --spawn-rate 50 --run-time 300s --headless \
    --html=report_heavy.html

# B∆∞·ªõc 7: Stress test - t√¨m breaking point
locust -f load_test_locust.py --host=http://localhost:8000 \
    --users 1000 --spawn-rate 100 --run-time 600s --headless \
    --html=report_stress.html
```

---

## Troubleshooting

### L·ªói: Connection refused

```
‚ùå requests.exceptions.ConnectionError: ('Connection aborted.')
```

**Gi·∫£i ph√°p**: Server ch∆∞a ch·∫°y, kh·ªüi ƒë·ªông server tr∆∞·ªõc

### L·ªói: Too many open files

```
‚ùå OSError: [Errno 24] Too many open files
```

**Gi·∫£i ph√°p**: TƒÉng file descriptor limit

```bash
ulimit -n 10000
```

### L·ªói: OpenAI Rate Limit

```
‚ùå openai.error.RateLimitError: Rate limit exceeded
```

**Gi·∫£i ph√°p**:

- Gi·∫£m s·ªë concurrent users
- Th√™m retry logic
- Upgrade OpenAI tier

---

## K·∫øt lu·∫≠n

V·ªõi 4 ph∆∞∆°ng ph√°p tr√™n, b·∫°n c√≥ th·ªÉ:

1. ‚úÖ **Locust**: Best cho production testing, c√≥ UI ƒë·∫πp
2. ‚úÖ **Simple Script**: Best cho quick tests, d·ªÖ customize
3. ‚úÖ **Apache Bench**: Best cho simple benchmarks
4. ‚úÖ **k6**: Best cho CI/CD integration

**Khuy√™n d√πng workflow:**

1. B·∫Øt ƒë·∫ßu v·ªõi **Simple Script** ƒë·ªÉ test nhanh
2. D√πng **Locust** cho comprehensive testing
3. Integrate **k6** v√†o CI/CD pipeline

**M·ª•c ti√™u:**

- T√¨m ra h·ªá th·ªëng ch·ªãu ƒë∆∞·ª£c bao nhi√™u concurrent users
- Identify bottlenecks
- Optimize v√† test l·∫°i
- Document results ƒë·ªÉ reference sau n√†y

---

## Resources

- [Locust Documentation](https://docs.locust.io/)
- [k6 Documentation](https://k6.io/docs/)
- [Apache Bench Manual](https://httpd.apache.org/docs/2.4/programs/ab.html)
- [FastAPI Performance Tips](https://fastapi.tiangolo.com/deployment/concepts/)
